{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TriTopic: Complete Feature Demo\n",
    "\n",
    "This notebook demonstrates **every feature** of the TriTopic library using the 20 Newsgroups dataset. It serves as a practical reference for how to use TriTopic in your own projects.\n",
    "\n",
    "**What we will cover:**\n",
    "\n",
    "1. Loading and preparing a news dataset\n",
    "2. Fitting a model with dimensionality reduction and iterative refinement\n",
    "3. Inspecting topics: keywords, representative documents, topic info\n",
    "4. Soft topic assignments (probabilities)\n",
    "5. Outlier reduction (embedding and neighbor strategies)\n",
    "6. Topic merging (`reduce_topics` and `merge_topics`)\n",
    "7. Evaluation metrics\n",
    "8. All 5 visualization types\n",
    "9. Predicting topics for new documents (hard labels + soft probabilities)\n",
    "10. LLM-powered and rule-based topic labeling\n",
    "11. Custom configuration\n",
    "12. Save and load\n",
    "13. Comparison with ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "We use scikit-learn's **20 Newsgroups** dataset, selecting 6 distinct categories to get clear, well-separated topics. We remove headers, footers, and quotes so the model works on article body text only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Select 6 distinct categories\n",
    "categories = [\n",
    "    \"sci.med\",\n",
    "    \"sci.space\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"rec.autos\",\n",
    "    \"comp.graphics\",\n",
    "    \"talk.politics.guns\",\n",
    "]\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset=\"all\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    categories=categories,\n",
    ")\n",
    "\n",
    "documents = newsgroups.data\n",
    "true_labels = newsgroups.target\n",
    "target_names = newsgroups.target_names\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {len(target_names)} categories\")\n",
    "print(f\"Categories: {target_names}\")\n",
    "print(f\"\\nSample document (first 300 chars):\\n{documents[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some newsgroup posts are very short or empty after header removal. Let's filter to documents with at least 50 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out very short documents\n",
    "mask = np.array([len(doc.strip()) >= 50 for doc in documents])\n",
    "documents = [doc for doc, keep in zip(documents, mask) if keep]\n",
    "true_labels = true_labels[mask]\n",
    "\n",
    "print(f\"After filtering: {len(documents)} documents\")\n",
    "print(f\"Documents per category:\")\n",
    "for i, name in enumerate(target_names):\n",
    "    print(f\"  {name}: {(true_labels == i).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fitting the Model\n",
    "\n",
    "We create a TriTopic model with default settings. This will:\n",
    "1. Encode documents with `all-MiniLM-L6-v2` (384d embeddings)\n",
    "2. Reduce to 10d with UMAP for better kNN neighbor quality\n",
    "3. Build a hybrid multi-view graph (semantic + lexical)\n",
    "4. Run consensus Leiden clustering (10 runs)\n",
    "5. Iteratively refine embeddings until convergence\n",
    "6. Extract c-TF-IDF keywords and compute soft probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritopic import TriTopic\n",
    "\n",
    "model = TriTopic(\n",
    "    n_neighbors=15,\n",
    "    use_iterative_refinement=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "labels = model.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Inspecting Topics\n",
    "\n",
    "After fitting, we can explore the discovered topics through several interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Topic overview table\n",
    "\n",
    "`get_topic_info()` returns a DataFrame with topic IDs, sizes, keywords, labels, and coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = model.get_topic_info()\n",
    "topic_df[[\"Topic\", \"Size\", \"Keywords\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Single topic detail\n",
    "\n",
    "`get_topic(topic_id)` returns a `TopicInfo` object with all fields: keywords, scores, representative doc indices, centroid, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the largest non-outlier topic\n",
    "largest_topic = [t for t in model.topics_ if t.topic_id != -1][0]\n",
    "topic = model.get_topic(largest_topic.topic_id)\n",
    "\n",
    "print(f\"Topic {topic.topic_id}\")\n",
    "print(f\"  Size: {topic.size} documents\")\n",
    "print(f\"  Top 10 keywords: {topic.keywords}\")\n",
    "print(f\"  Keyword scores:  {[f'{s:.4f}' for s in topic.keyword_scores]}\")\n",
    "print(f\"  Centroid shape:  {topic.centroid.shape}\")\n",
    "print(f\"  Representative doc indices: {topic.representative_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Representative documents\n",
    "\n",
    "`get_representative_docs()` returns the documents closest to the topic centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show representative docs for the first 3 non-outlier topics\n",
    "for topic_info in model.topics_[:5]:\n",
    "    if topic_info.topic_id == -1:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Topic {topic_info.topic_id} | Keywords: {', '.join(topic_info.keywords[:5])}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    rep_docs = model.get_representative_docs(topic_info.topic_id, n_docs=2)\n",
    "    for idx, text in rep_docs:\n",
    "        print(f\"  [Doc {idx}] {text[:200].strip()}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Dimensionality Reduction\n",
    "\n",
    "By default, TriTopic reduces embeddings from 384d to 10d using UMAP before building the kNN graph. This combats the curse of dimensionality. Full-dimensional embeddings are preserved for centroid computation and keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Full embeddings shape:    {model.embeddings_.shape}\")\n",
    "print(f\"Reduced embeddings shape: {model.reduced_embeddings_.shape}\")\n",
    "print(f\"Topic centroids shape:    {model.topic_embeddings_.shape}  (computed from full embeddings)\")\n",
    "print(f\"\\nDim reduction method: {model.config.dim_reduction_method}\")\n",
    "print(f\"Target dimensions:    {model.config.reduced_dims}\")\n",
    "print(f\"UMAP min_dist:        {model.config.umap_min_dist}  (0.0 = optimized for clustering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Soft Topic Assignments (Probabilities)\n",
    "\n",
    "Every document receives a probability distribution over all topics, not just a hard label. This is computed via cosine similarity to topic centroids, followed by softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Probabilities shape: {model.probabilities_.shape}\")\n",
    "print(f\"  (n_documents={model.probabilities_.shape[0]}, n_topics={model.probabilities_.shape[1]})\")\n",
    "print(f\"\\nRow sums (should be ~1.0):\")\n",
    "print(f\"  First 5 rows: {model.probabilities_[:5].sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the probability distribution for a specific document\n",
    "doc_idx = 0\n",
    "non_outlier_ids = [t.topic_id for t in model.topics_ if t.topic_id != -1]\n",
    "\n",
    "print(f\"Document {doc_idx}: \\\"{documents[doc_idx][:100].strip()}...\\\"\")\n",
    "print(f\"Hard label: {model.labels_[doc_idx]}\")\n",
    "print(f\"\\nSoft probabilities:\")\n",
    "\n",
    "for i, tid in enumerate(non_outlier_ids):\n",
    "    prob = model.probabilities_[doc_idx, i]\n",
    "    topic = model.get_topic(tid)\n",
    "    bar = '#' * int(prob * 50)\n",
    "    print(f\"  Topic {tid:2d} ({', '.join(topic.keywords[:3]):30s}): {prob:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Outlier Reduction\n",
    "\n",
    "Leiden clustering combined with small-cluster removal often produces outliers. TriTopic provides two strategies to reassign them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outliers_before = int((model.labels_ == -1).sum())\n",
    "n_total = len(model.labels_)\n",
    "print(f\"Before outlier reduction: {n_outliers_before} outliers ({100*n_outliers_before/n_total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Embeddings\n",
    "\n",
    "Each outlier is assigned to the topic whose centroid is most similar (cosine similarity), if the similarity exceeds a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a low threshold for aggressive reassignment\n",
    "model.reduce_outliers(strategy=\"embeddings\", threshold=0.05)\n",
    "\n",
    "n_outliers_after = int((model.labels_ == -1).sum())\n",
    "print(f\"After 'embeddings' strategy: {n_outliers_after} outliers ({100*n_outliers_after/n_total:.1f}%)\")\n",
    "print(f\"Reassigned: {n_outliers_before - n_outliers_after} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Neighbors\n",
    "\n",
    "Each remaining outlier is assigned by majority vote of its k nearest non-outlier neighbors. This is threshold-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_before_neighbors = int((model.labels_ == -1).sum())\n",
    "\n",
    "model.reduce_outliers(strategy=\"neighbors\")\n",
    "\n",
    "n_after_neighbors = int((model.labels_ == -1).sum())\n",
    "print(f\"After 'neighbors' strategy: {n_after_neighbors} outliers ({100*n_after_neighbors/n_total:.1f}%)\")\n",
    "print(f\"Reassigned: {n_before_neighbors - n_after_neighbors} additional documents\")\n",
    "print(f\"\\nTotal outliers reduced: {n_outliers_before} -> {n_after_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that downstream state was refreshed\n",
    "print(\"Updated topic sizes after outlier reduction:\")\n",
    "for t in model.topics_:\n",
    "    label = \"Outliers\" if t.topic_id == -1 else f\"Topic {t.topic_id}\"\n",
    "    print(f\"  {label}: {t.size} docs\")\n",
    "\n",
    "print(f\"\\nProbabilities shape: {model.probabilities_.shape}  (recomputed automatically)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Topic Merging\n",
    "\n",
    "TriTopic offers two ways to merge topics after fitting: automatic merging to a target count, and manual merging of specific topic IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_before = len([t for t in model.topics_ if t.topic_id != -1])\n",
    "print(f\"Topics before merging: {n_topics_before}\")\n",
    "print(f\"Topic IDs: {[t.topic_id for t in model.topics_ if t.topic_id != -1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Automatic: `reduce_topics(n_topics)`\n",
    "\n",
    "Iteratively merges the two most cosine-similar topic centroids until the target count is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 6 topics (matching our 6 ground-truth categories)\n",
    "model.reduce_topics(6)\n",
    "\n",
    "n_topics_after = len([t for t in model.topics_ if t.topic_id != -1])\n",
    "print(f\"Topics after reduce_topics(6): {n_topics_after}\")\n",
    "print()\n",
    "model.get_topic_info()[[\"Topic\", \"Size\", \"Keywords\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Manual: `merge_topics(topic_ids)`\n",
    "\n",
    "Merge specific topic IDs together. The largest topic's ID is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate manual merge on two topics (pick the two smallest)\n",
    "non_outlier = [t for t in model.topics_ if t.topic_id != -1]\n",
    "if len(non_outlier) >= 3:\n",
    "    # Sort by size to find the two smallest\n",
    "    smallest = sorted(non_outlier, key=lambda t: t.size)[:2]\n",
    "    ids_to_merge = [t.topic_id for t in smallest]\n",
    "    print(f\"Merging topics {ids_to_merge} (sizes: {[t.size for t in smallest]})\")\n",
    "\n",
    "    model.merge_topics(ids_to_merge)\n",
    "\n",
    "    n_after_merge = len([t for t in model.topics_ if t.topic_id != -1])\n",
    "    print(f\"Topics after manual merge: {n_after_merge}\")\n",
    "    print()\n",
    "    print(model.get_topic_info()[[\"Topic\", \"Size\", \"Keywords\"]].to_string())\n",
    "else:\n",
    "    print(\"Not enough topics to demonstrate manual merge (need >= 3).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluation\n",
    "\n",
    "`evaluate()` computes coherence, diversity, stability, and outlier ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate()\n",
    "\n",
    "print(\"\\nMetrics dictionary:\")\n",
    "for key, value in metrics.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {key:20s}: {value:.4f}\" if isinstance(value, float) else f\"  {key:20s}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key:20s}: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional standalone metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritopic.utils.metrics import compute_silhouette, compute_downstream_score\n",
    "\n",
    "# Silhouette score (cluster separation quality)\n",
    "sil = compute_silhouette(model.embeddings_, model.labels_)\n",
    "print(f\"Silhouette score: {sil:.4f}\")\n",
    "\n",
    "# Downstream classification F1 (using ground truth)\n",
    "f1 = compute_downstream_score(\n",
    "    model.embeddings_, model.labels_, true_labels, task=\"classification\"\n",
    ")\n",
    "print(f\"Downstream F1 (macro): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualizations\n",
    "\n",
    "TriTopic provides 5 interactive visualization types, all powered by Plotly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9a. Document map\n",
    "\n",
    "2D UMAP projection where each point is a document, colored by topic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model.visualize(\n",
    "    method=\"umap\",\n",
    "    show_outliers=True,\n",
    "    title=\"TriTopic Document Map (20 Newsgroups)\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Topic keywords\n",
    "\n",
    "Horizontal bar charts showing the top keywords and their importance scores for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model.visualize_topics(\n",
    "    n_keywords=8,\n",
    "    title=\"Topic Keywords Overview\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9c. Topic hierarchy (dendrogram)\n",
    "\n",
    "Shows how topics relate to each other based on cosine distance between centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model.visualize_hierarchy(\n",
    "    title=\"Topic Hierarchy (Dendrogram)\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9d. Topic similarity heatmap\n",
    "\n",
    "Cosine similarity between all topic centroid pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritopic import TopicVisualizer\n",
    "\n",
    "viz = TopicVisualizer()\n",
    "fig = viz.plot_topic_similarity(\n",
    "    model.topic_embeddings_,\n",
    "    model.topics_,\n",
    "    title=\"Topic Similarity Matrix\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9e. Topics over time\n",
    "\n",
    "Stacked area chart showing topic prevalence over time. Since 20 Newsgroups doesn't have timestamps, we generate synthetic dates for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic timestamps for demonstration\n",
    "np.random.seed(42)\n",
    "fake_dates = pd.date_range(\"2024-01-01\", periods=12, freq=\"MS\")  # 12 months\n",
    "timestamps = np.random.choice(fake_dates, size=len(documents))\n",
    "\n",
    "fig = viz.plot_topic_over_time(\n",
    "    labels=model.labels_,\n",
    "    timestamps=timestamps,\n",
    "    topics=model.topics_,\n",
    "    title=\"Topic Distribution Over Time (synthetic dates)\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Predicting Topics for New Documents\n",
    "\n",
    "After fitting, you can assign topics to documents the model has never seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10a. Hard labels via `transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = [\n",
    "    \"NASA's Perseverance rover discovered organic molecules on Mars, raising hopes for ancient microbial life.\",\n",
    "    \"The Yankees clinched the division title with a walk-off home run in the bottom of the ninth inning.\",\n",
    "    \"NVIDIA announced its next-generation GPU with real-time ray tracing and AI-accelerated rendering.\",\n",
    "    \"A new study in the Lancet shows promising results for an mRNA-based cancer vaccine in phase 3 trials.\",\n",
    "    \"The debate over gun control legislation intensified after another mass shooting incident.\",\n",
    "    \"Ford revealed its all-electric F-150 Lightning truck with 300 miles of range and fast charging.\",\n",
    "]\n",
    "\n",
    "new_labels = model.transform(new_documents)\n",
    "\n",
    "print(\"Predicted topics for new documents:\\n\")\n",
    "for doc, label in zip(new_documents, new_labels):\n",
    "    if label == -1:\n",
    "        topic_keywords = \"(outlier)\"\n",
    "    else:\n",
    "        topic = model.get_topic(label)\n",
    "        topic_keywords = \", \".join(topic.keywords[:4])\n",
    "    print(f\"  Topic {label:2d} [{topic_keywords}]\")\n",
    "    print(f\"    -> {doc[:90]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10b. Soft probabilities via `transform_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_proba = model.transform_proba(new_documents)\n",
    "\n",
    "print(f\"Probabilities shape: {new_proba.shape}\")\n",
    "print(f\"Row sums: {new_proba.sum(axis=1)}\\n\")\n",
    "\n",
    "# Show probabilities as a DataFrame\n",
    "non_outlier_ids = [t.topic_id for t in model.topics_ if t.topic_id != -1]\n",
    "proba_df = pd.DataFrame(\n",
    "    new_proba,\n",
    "    columns=[f\"Topic {tid}\" for tid in non_outlier_ids],\n",
    "    index=[f\"Doc {i}: {doc[:50]}...\" for i, doc in enumerate(new_documents)],\n",
    ")\n",
    "proba_df.style.background_gradient(cmap=\"YlOrRd\", axis=1).format(\"{:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Topic Labeling\n",
    "\n",
    "TriTopic supports LLM-powered labeling (Claude, GPT-4) and a simple rule-based fallback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11a. SimpleLabeler (no API key needed)\n",
    "\n",
    "Creates labels from the top keywords. Good for quick exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritopic import SimpleLabeler\n",
    "\n",
    "simple_labeler = SimpleLabeler(n_words=3)\n",
    "model.generate_labels(simple_labeler)\n",
    "\n",
    "print(\"Topics with simple labels:\\n\")\n",
    "for topic in model.topics_:\n",
    "    if topic.topic_id == -1:\n",
    "        continue\n",
    "    print(f\"  Topic {topic.topic_id}: {topic.label}\")\n",
    "    print(f\"    {topic.description}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11b. LLM Labeler (requires API key)\n",
    "\n",
    "Uncomment and fill in your API key to generate high-quality labels with Claude or GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Uncomment one of the options below ----\n",
    "\n",
    "# Option A: Claude (Anthropic)\n",
    "from tritopic import LLMLabeler\n",
    "labeler = LLMLabeler(\n",
    "    provider=\"anthropic\",\n",
    "    api_key=\"sk-ant...\",\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    language=\"english\",\n",
    "    domain_hint=\"news articles\",\n",
    ")\n",
    "model.generate_labels(labeler)\n",
    "\n",
    "# Option B: GPT-4 (OpenAI)\n",
    "# from tritopic import LLMLabeler\n",
    "# labeler = LLMLabeler(\n",
    "#     provider=\"openai\",\n",
    "#     api_key=\"sk-...\",\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     language=\"english\",\n",
    "# )\n",
    "# model.generate_labels(labeler)\n",
    "\n",
    "# # View results\n",
    "model.get_topic_info()[[\"Topic\", \"Label\", \"Description\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Custom Configuration\n",
    "\n",
    "TriTopic is highly configurable. Here we show how to create a model with a custom `TriTopicConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritopic import TriTopicConfig\n",
    "\n",
    "custom_config = TriTopicConfig(\n",
    "    # Use a stronger embedding model\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",  # swap to \"all-mpnet-base-v2\" for higher quality\n",
    "    embedding_batch_size=64,\n",
    "\n",
    "    # Dimensionality reduction\n",
    "    use_dim_reduction=True,\n",
    "    reduced_dims=8,\n",
    "    dim_reduction_method=\"umap\",\n",
    "    umap_n_neighbors=20,\n",
    "    umap_min_dist=0.0,\n",
    "\n",
    "    # Graph construction\n",
    "    n_neighbors=20,\n",
    "    graph_type=\"hybrid\",\n",
    "    snn_weight=0.4,\n",
    "\n",
    "    # Multi-view fusion\n",
    "    use_lexical_view=True,\n",
    "    semantic_weight=0.6,\n",
    "    lexical_weight=0.4,\n",
    "\n",
    "    # Clustering\n",
    "    resolution=1.0,\n",
    "    n_consensus_runs=15,\n",
    "    min_cluster_size=10,\n",
    "\n",
    "    # Iterative refinement\n",
    "    use_iterative_refinement=True,\n",
    "    max_iterations=7,\n",
    "    convergence_threshold=0.97,\n",
    "\n",
    "    # Keywords\n",
    "    keyword_method=\"ctfidf\",\n",
    "    n_keywords=15,\n",
    "    n_representative_docs=5,\n",
    "\n",
    "    # Misc\n",
    "    outlier_threshold=0.1,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Custom config created. Key settings:\")\n",
    "print(f\"  Dim reduction:  {custom_config.reduced_dims}d ({custom_config.dim_reduction_method})\")\n",
    "print(f\"  Graph:          {custom_config.graph_type} (k={custom_config.n_neighbors})\")\n",
    "print(f\"  Consensus:      {custom_config.n_consensus_runs} runs\")\n",
    "print(f\"  Refinement:     max {custom_config.max_iterations} iterations\")\n",
    "print(f\"  Keywords:       {custom_config.keyword_method} (n={custom_config.n_keywords})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model with the custom config\n",
    "# (uses the same embeddings we already computed to save time)\n",
    "model_custom = TriTopic(config=custom_config)\n",
    "labels_custom = model_custom.fit_transform(documents, embeddings=model.embeddings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom.get_topic_info()[[\"Topic\", \"Size\", \"Keywords\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Save and Load\n",
    "\n",
    "All model state is preserved: embeddings, reduced embeddings, probabilities, the fitted UMAP reducer, topics, labels, and keyword extractor state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path = \"tritopic_demo_model.pkl\"\n",
    "\n",
    "# Save\n",
    "model.save(save_path)\n",
    "file_size_mb = os.path.getsize(save_path) / (1024 * 1024)\n",
    "print(f\"Model saved to {save_path} ({file_size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "loaded_model = TriTopic.load(save_path)\n",
    "\n",
    "print(f\"Loaded model: {loaded_model}\")\n",
    "print(f\"  Labels shape:              {loaded_model.labels_.shape}\")\n",
    "print(f\"  Embeddings shape:          {loaded_model.embeddings_.shape}\")\n",
    "print(f\"  Reduced embeddings shape:  {loaded_model.reduced_embeddings_.shape}\")\n",
    "print(f\"  Probabilities shape:       {loaded_model.probabilities_.shape}\")\n",
    "print(f\"  Topic centroids shape:     {loaded_model.topic_embeddings_.shape}\")\n",
    "print(f\"  Dim reducer present:       {loaded_model._dim_reducer is not None}\")\n",
    "print(f\"  Number of topics:          {len([t for t in loaded_model.topics_ if t.topic_id != -1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify loaded model produces the same predictions\n",
    "test_docs = [\"Hubble telescope captured images of a distant galaxy\"]\n",
    "original_pred = model.transform(test_docs)\n",
    "loaded_pred = loaded_model.transform(test_docs)\n",
    "\n",
    "print(f\"Original model prediction:  Topic {original_pred[0]}\")\n",
    "print(f\"Loaded model prediction:    Topic {loaded_pred[0]}\")\n",
    "print(f\"Match: {np.array_equal(original_pred, loaded_pred)}\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(save_path)\n",
    "print(f\"\\nCleaned up {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Comparison with Ground Truth\n",
    "\n",
    "Since we know the true newsgroup categories, we can measure how well TriTopic's discovered topics align with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# Filter out outliers for a fair comparison\n",
    "non_outlier_mask = model.labels_ != -1\n",
    "pred = model.labels_[non_outlier_mask]\n",
    "gt = true_labels[non_outlier_mask]\n",
    "\n",
    "ari = adjusted_rand_score(gt, pred)\n",
    "nmi = normalized_mutual_info_score(gt, pred)\n",
    "\n",
    "print(f\"Comparison with ground truth ({non_outlier_mask.sum()}/{len(model.labels_)} non-outlier docs):\")\n",
    "print(f\"  Adjusted Rand Index (ARI):       {ari:.4f}\")\n",
    "print(f\"  Normalized Mutual Info (NMI):     {nmi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation: how do discovered topics map to true categories?\n",
    "ct = pd.crosstab(\n",
    "    pd.Series(gt, name=\"True Category\").map(dict(enumerate(target_names))),\n",
    "    pd.Series(pred, name=\"Predicted Topic\"),\n",
    ")\n",
    "ct.style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Summary\n",
    "\n",
    "This notebook demonstrated all major features of TriTopic:\n",
    "\n",
    "| Feature | Method / Attribute | Section |\n",
    "|---|---|---|\n",
    "| Fit model | `fit()`, `fit_transform()` | 2 |\n",
    "| Topic inspection | `get_topic_info()`, `get_topic()`, `get_representative_docs()` | 3 |\n",
    "| Dimensionality reduction | `reduced_embeddings_`, config `use_dim_reduction` | 4 |\n",
    "| Soft assignments | `probabilities_`, `transform_proba()` | 5 |\n",
    "| Outlier reduction | `reduce_outliers(strategy=...)` | 6 |\n",
    "| Topic merging | `reduce_topics()`, `merge_topics()` | 7 |\n",
    "| Evaluation | `evaluate()` | 8 |\n",
    "| Visualizations | `visualize()`, `visualize_topics()`, `visualize_hierarchy()`, `plot_topic_similarity()`, `plot_topic_over_time()` | 9 |\n",
    "| Prediction | `transform()`, `transform_proba()` | 10 |\n",
    "| Topic labeling | `generate_labels()` with `SimpleLabeler` / `LLMLabeler` | 11 |\n",
    "| Custom config | `TriTopicConfig(...)` | 12 |\n",
    "| Save / Load | `save()`, `TriTopic.load()` | 13 |\n",
    "| Ground truth comparison | sklearn metrics | 14 |\n",
    "\n",
    "For more details, see the [README](../README.md) and [API documentation](https://tritopic.readthedocs.io)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
